{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYDE 556/750 --- Assignment 1\n",
    "**Student ID: 20823934**\n",
    "\n",
    "*Note:* Please include your numerical student ID only, do *not* include your name.\n",
    "\n",
    "*Note:* Refer to the [PDF](https://github.com/celiasmith/syde556-f22/raw/master/assignments/assignment_01/syde556_assignment_01.pdf) for the full instructions (including some hints), this notebook contains abbreviated instructions only. Cells you need to fill out are marked with a \"writing hand\" symbol. Of course, you can add new cells in between the instructions, but please leave the instructions intact to facilitate marking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy and matplotlib -- you shouldn't need any other libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize # For question 2.1b)\n",
    "\n",
    "# Fix the numpy random seed for reproducible results\n",
    "np.random.seed(18945)\n",
    "\n",
    "# Some formating options\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Representation of Scalars\n",
    "\n",
    "## 1.1 Basic encoding and decoding\n",
    "\n",
    "**a) Computing gain and bias.** In general, for a neuron model $a = G[J]$ (and assuming that the inverse $J = G^{-1}[a]$ exists), solve the following system of equations to compute the gain $\\alpha$, and the bias $J^\\mathrm{bias}$ given a maximum rate $a^\\mathrm{max}$ and an $x$-intercept $\\xi$.\n",
    "\n",
    "$$a^\\mathrm{max} = G[\\alpha + J^\\mathrm{bias}] \\,, \\quad\\quad 0 = G[\\alpha \\xi + J^\\mathrm{bias}] \\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ \\<YOUR SOLUTION HERE\\>\n",
    "\n",
    "$$a^\\mathrm{max} = G[\\alpha + J^\\mathrm{bias}] \\,, \\quad\\quad 0 = G[\\alpha \\xi + J^\\mathrm{bias}] \\implies$$\n",
    "\n",
    "Take the inverse of both sides of the equations:\n",
    "\n",
    "$$G^{-1}[a^\\mathrm{max}] = \\alpha + J^\\mathrm{bias} \\,, \\quad\\quad G^{-1}[0] = \\alpha \\xi + J^\\mathrm{bias} \\implies$$\n",
    "\n",
    "Shift the terms so $J^\\mathrm{bias}$ is on the left side of both equations. Now the right sides equal and $J^\\mathrm{bias}$ can be ignored:\n",
    "\n",
    "$$J^\\mathrm{bias} = G^{-1}[a^\\mathrm{max}] - \\alpha \\,, \\quad\\quad J^\\mathrm{bias} = G^{-1}[0] - \\alpha \\xi \\implies$$\n",
    "\n",
    "$$G^{-1}[a^\\mathrm{max}] - \\alpha = G^{-1}[0] - \\alpha \\xi \\implies$$\n",
    "\n",
    "Shift the terms so $G$ is on the left and $\\alpha$ is on the right:\n",
    "\n",
    "$$G^{-1}[a^\\mathrm{max}] - G^{-1}[0] = \\alpha - \\alpha \\xi = \\alpha (1 - \\xi) \\implies$$\n",
    "\n",
    "Now isolate $\\alpha$ by dividing both sides by $(1 - \\xi)$:\n",
    "\n",
    "$$\\therefore \\alpha = \\frac{G^{-1}[a^\\mathrm{max}] - G^{-1}[0]}{1 - \\xi}$$\n",
    "\n",
    "Plug the result of $\\alpha$ into a previous equation to find $J^\\mathrm{bias}$:\n",
    "\n",
    "$$J^\\mathrm{bias} = G^{-1}[a^\\mathrm{max}] - \\frac{G^{-1}[a^\\mathrm{max}] - G^{-1}[0]}{1 - \\xi} \\implies$$\n",
    "\n",
    "$$J^\\mathrm{bias} = \\frac{G^{-1}[a^\\mathrm{max}](1 - \\xi) - G^{-1}[a^\\mathrm{max}] + G^{-1}[0]}{1 - \\xi} \\implies$$\n",
    "\n",
    "$$J^\\mathrm{bias} = \\frac{G^{-1}[a^\\mathrm{max}] - \\xi G^{-1}[a^\\mathrm{max}] - G^{-1}[a^\\mathrm{max}] + G^{-1}[0]}{1 - \\xi} \\implies$$\n",
    "\n",
    "$$J^\\mathrm{bias} = \\frac{-\\xi G^{-1}[a^\\mathrm{max}] + G^{-1}[0]}{1 - \\xi} \\implies$$\n",
    "\n",
    "Flip the negative signs:\n",
    "\n",
    "$$\\therefore J^\\mathrm{bias} = \\frac{\\xi G^{-1}[a^\\mathrm{max}] - G^{-1}[0]}{\\xi - 1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, simplify these equations for the specific case $G[J] = \\max(J, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ \\<YOUR SOLUTION HERE\\>\n",
    "\n",
    "For the ReLU function $G[J] = \\max(J, 0)$, the inverse only exists for $J \\ge 0$. In this region, the inverse of ReLU is also ReLU: $G^{-1}[J] = G[J] = max(J, 0)$. In addition, we substitute $J_{th}$ in place of $G^{-1}[0]$:\n",
    "\n",
    "$$\\alpha = \\frac{max(a^\\mathrm{max}, 0) - J_{th}}{1 - \\xi}$$\n",
    "\n",
    "$$J^\\mathrm{bias} = \\frac{\\xi max(a^\\mathrm{max}, 0) - J_{th}}{\\xi - 1}$$\n",
    "\n",
    "We can assume that $J_{th} = 0$ because the only input that results in a ReLU of 0 in the invertible region $J \\ge 0$ is 0:\n",
    "\n",
    "$$\\alpha = \\frac{max(a^\\mathrm{max}, 0)}{1 - \\xi}$$\n",
    "\n",
    "$$J^\\mathrm{bias} = \\frac{\\xi max(a^\\mathrm{max}, 0)}{\\xi - 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Neuron tuning curves.** Plot the neuron tuning curves $a_i(x)$ for 16 randomly generated neurons following the intercept and maximum rate distributions described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>\n",
    "\n",
    "\"\"\" Sampling the random variables \"\"\"\n",
    "\n",
    "# Set a reliable seed\n",
    "np.random.seed(30)\n",
    "\n",
    "# Sample the x-axis at a resolution of 0.05\n",
    "x = np.linspace(-1, 1, 41)\n",
    "\n",
    "# Sample firing rates a^max uniformly between 100Hz and 200Hz at x=1\n",
    "a_max = np.random.uniform(low=100, high=200, size=16)\n",
    "\n",
    "# Sample x-intercepts \\xi uniformly between -0.95 and 0.95\n",
    "xi = np.random.uniform(low=-0.95, high=0.95, size=16)\n",
    "\n",
    "# Randomly set encoder e to either +1 or -1 for each neuron\n",
    "e = np.random.choice([-1, 1], size=16)\n",
    "\n",
    "\"\"\" Computing alpha and J^bias \"\"\"\n",
    "\n",
    "alpha = (np.maximum(a_max, 0)) / (1 - xi)\n",
    "J_bias = (xi * np.maximum(a_max, 0)) / (xi - 1)\n",
    "\n",
    "\"\"\" Calculating the tuning curves \"\"\"\n",
    "\n",
    "A = []\n",
    "for i in range(16):\n",
    "    a = [np.maximum(e[i] * alpha[i] * x_n + J_bias[i], 0) for x_n in x]\n",
    "    A.append(a)\n",
    "A = np.array(A)\n",
    "\n",
    "\"\"\" Plotting the curves \"\"\"\n",
    "\n",
    "def plot_A(x, A):\n",
    "    for i in range(A.shape[0]):\n",
    "        plt.plot(x, A[i])\n",
    "\n",
    "    plt.title(f\"{A.shape[0]} Tuning Curves\")\n",
    "    plt.xlabel(\"Represented Value x\")\n",
    "    plt.ylabel(\"Firing Rate (Hz)\")\n",
    "    plt.show()\n",
    "\n",
    "plot_A(x, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Computing identity decoders.** Compute the optimal identity decoder $\\vec d$ for those 16 neurons (as shown in class). Report the value of the individual decoder coefficients. Compute $d$ using the matrix notation mentioned in the course notes. Do not apply any regularization. $A$ is the matrix of activities (the same data used to generate the plot in 1.1b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>\n",
    "\n",
    "# Decoding via matrix notation\n",
    "D = A @ x @ np.linalg.inv(A @ A.T)\n",
    "D\n",
    "\n",
    "# Decoding using the Python code from lecture notes\n",
    "# D = np.linalg.lstsq(A.T, x.T, rcond=None)[0].T\n",
    "# D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) Evaluating decoding errors.** Compute and plot $\\hat{x}=\\sum_i d_i a_i(x)$. Overlay on the plot the line $y=x$. Make a separate plot of $x-\\hat{x}$ to see what the error looks like. Report the Root Mean Squared Error (RMSE) value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>\n",
    "\n",
    "def plot_decoder_error(x, D, A):\n",
    "    # Compute the decoded value\n",
    "    x_hat = D @ A\n",
    "\n",
    "    # Report the RMSE\n",
    "    rmse = np.sqrt(np.mean((x_hat - x)**2))\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "\n",
    "    # Make two plots side by side\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "    \"\"\" Compare x_hat to x \"\"\"\n",
    "\n",
    "    axs[0].plot(x, x, label=\"Ideal\")\n",
    "    axs[0].plot(x, x_hat, label=\"Decoded\")\n",
    "    axs[0].legend()\n",
    "    axs[0].set_title(\"Ideal and Decoded Value\")\n",
    "    axs[0].set_xlabel(\"Represented Value x\")\n",
    "    axs[0].set_ylabel(\"Decoded Value x_hat\")\n",
    "\n",
    "    \"\"\" Plot the error \"\"\"\n",
    "\n",
    "    axs[1].plot(x, x_hat - x)\n",
    "    axs[1].set_title(f\"Error (RMSE = {round(rmse, 3)})\")\n",
    "    axs[1].set_xlabel(\"Represented Value x\")\n",
    "    axs[1].set_ylabel(\"Decoder Error\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_decoder_error(x, D, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) Decoding under noise.** Now try decoding under noise. Add random normally distributed noise to $a$ and decode again. The noise is a random variable with mean $\\mu=0$ and standard deviation of $\\sigma=0.2 \\max(A)$ (where $\\max(A)$ is the maximum firing rate of all the neurons). Resample this variable for every different $x$ value for every different neuron. Create all the same plots as in part d). Report the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>\n",
    "\n",
    "\"\"\" Adding noise to A \"\"\"\n",
    "\n",
    "# Set a reliable seed\n",
    "np.random.seed(100)\n",
    "\n",
    "# Generating the noise\n",
    "mu = 0\n",
    "sigma = 0.2 * np.max(A)\n",
    "noise = np.random.normal(mu, sigma, A.shape)\n",
    "\n",
    "# Add the noise but don't allow negative firing rates\n",
    "A_noisy = np.maximum(A + noise, 0)\n",
    "plot_A(x, A_noisy)\n",
    "\n",
    "\"\"\" Decode using the noisy tuning curves \"\"\"\n",
    "\n",
    "# Plot the error\n",
    "plot_decoder_error(x, D, A_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f) Accounting for decoder noise.** Recompute the decoder $\\vec d$ taking noise into account (i.e., apply the appropriate regularization, as shown in class). Show how these decoders behave when decoding both with and without noise added to $a$ by making the same plots as in d) and e). Report the RMSE for all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>\n",
    "\n",
    "# Calculate the parameters\n",
    "n = A_noisy.shape[0]  # Number of tuning curves a\n",
    "N = A_noisy.shape[1]  # Number of samples x\n",
    "\n",
    "# Decoding + regularization via matrix notation\n",
    "D_reg = A @ x @ np.linalg.inv(A @ A.T + N * np.square(sigma) * np.eye(n))\n",
    "\n",
    "# Decoding + regularization using the Python code from lecture notes\n",
    "# D_reg = np.linalg.lstsq(A @ A.T + N * np.square(sigma) * np.eye(n), A @ x.T, rcond=None)[0].T\n",
    "\n",
    "print(\"Regularized D, noisy A\")\n",
    "plot_decoder_error(x, D_reg, A_noisy)\n",
    "\n",
    "print(\"Regularized D, noise-less A\")\n",
    "plot_decoder_error(x, D_reg, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g) Interpretation.** Show a 2x2 table of the four RMSE values reported in parts d), e), and f). This should show the effects of adding noise and whether the decoders $d$ are computed taking noise into account. Write a few sentences commenting on what the table shows, i.e., what the effect of adding noise to the activities is with respect to the measured error and why accounting for noise when computing the decoders increases/decreases/does not change the measured RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ \\<YOUR SOLUTION HERE\\>\n",
    "\n",
    "Table 1 - RMSE for decoders with and without noise\n",
    "|          | A        | Noisy A |\n",
    "|----------|----------|----------|\n",
    "| **D**               | 0.001   | 0.313   |\n",
    "| **Regularized D**   | 0.040   | 0.187   |\n",
    "\n",
    "When noise is added to neural activities A, the RMSE increased by 1-2 orders of magnitude. The unregularized decoder D is very accurate for the noise-less A, but produces a lot of error for noisy A. On the other hand, the regularized D decreased the error for noisy A by half (column 2) but increased the error for the noise-less A (column 1). Overall, regularization is a good thing because the amount of error it reduced is much greater than the amount it introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Exploring sources of error\n",
    "\n",
    "**a) Exploring error due to distortion and noise.** Plot the error due to distortion $E_\\mathrm{dist}$ and the error due to noise $E_\\mathrm{noise}$ as a function of $n$, the number of neurons. Generate two different loglog plots (one for each type of error) with $n$ values of at least $[4, 8, 16, 32, 64, 128, 256, 512]$. For each $n$ value, do at least $5$ runs and average the results. For each run, different $\\alpha$, $J^\\mathrm{bias}$, and $e$ values should be generated for each neuron. Compute $d$ taking noise into account, with $\\sigma = 0.1 \\max(A)$. Show visually that the errors are proportional to $1/n$ or $1/n^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>\n",
    "\n",
    "# Calculate E_dist and E_noise for n neurons (1 run)\n",
    "def run(n=16, N=41):\n",
    "    # Sample new random variables\n",
    "    x = np.linspace(-1, 1, N)                               # Sample the x-axis\n",
    "    a_max = np.random.uniform(low=100, high=200, size=n)    # Sample firing rates\n",
    "    xi = np.random.uniform(low=-0.95, high=0.95, size=n)    # Sample x-intercepts\n",
    "    e = np.random.choice([-1, 1], size=n)                   # Set encoder directions\n",
    "\n",
    "    # Compute alpha and J^bias\n",
    "    alpha = (np.maximum(a_max, 0)) / (1 - xi)\n",
    "    J_bias = (xi * np.maximum(a_max, 0)) / (xi - 1)\n",
    "\n",
    "    # Calculate tuning curves for each sample\n",
    "    A = []\n",
    "    for i in range(n):\n",
    "        a = [np.maximum(e[i] * alpha[i] * x_i + J_bias[i], 0) for x_i in x]\n",
    "        A.append(a)\n",
    "    A = np.array(A)\n",
    "\n",
    "    # Add noise to A\n",
    "    sigma = 0.1 * np.max(A)\n",
    "    noise = np.random.normal(0, sigma, A.shape)\n",
    "    A_noisy = np.maximum(A + noise, 0)\n",
    "\n",
    "    # Compute D taking noise into account\n",
    "    D = A @ x @ np.linalg.inv(A @ A.T + N * np.square(sigma) * np.eye(n))\n",
    "\n",
    "    # Calculate the errors\n",
    "    x_hat = D @ A_noisy\n",
    "    E_dist = 0.5 * np.sum((x - x_hat)**2)\n",
    "    E_noise = 0.5 * sigma**2 * np.sum(D**2)\n",
    "\n",
    "    return E_dist, E_noise \n",
    "\n",
    "# Try different values of n\n",
    "ns = [2 ** i for i in range(2, 10)]\n",
    "avg_E_dist = []\n",
    "avg_E_noise = []\n",
    "for n in ns:\n",
    "    # Average the errors over 5 runs\n",
    "    E_dists = []\n",
    "    E_noises = []\n",
    "    for i in range(5):\n",
    "        E_dist, E_noise = run(n=n)\n",
    "        E_dists.append(E_dist)\n",
    "        E_noises.append(E_noise)\n",
    "    avg_E_dist.append(np.mean(E_dists))\n",
    "    avg_E_noise.append(np.mean(E_noises))\n",
    "\n",
    "# Make two plots side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "# Plot E_dist (proportional to 1/n^2)\n",
    "n_inv2 = n**-2\n",
    "axs[0].plot(n, n_inv2, label=\"1/n^2\")\n",
    "axs[0].plot(n, avg_E_dist, label=\"Neurons\")\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"Error due to static distortion E_dist\")\n",
    "axs[0].set_xlabel(\"Number of neurons n\")\n",
    "axs[0].set_ylabel(\"Square error\")\n",
    "\n",
    "# Plot E_noise (proportional to 1/n)\n",
    "n_inv = n**-1\n",
    "axs[1].plot(n, n_inv, label=\"1/n\")\n",
    "axs[1].plot(n, avg_E_noise, label=\"Neurons\")\n",
    "axs[1].legend()\n",
    "axs[1].set_title(\"Error due to noise E_noise\")\n",
    "axs[1].set_xlabel(\"Number of neurons n\")\n",
    "axs[1].set_ylabel(\"Square error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Adapting the noise level.** Repeat part a) with $\\sigma = 0.01 \\max(A)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Interpretation.** What does the difference between the graphs in a) and b) tell us about the sources of error in neural populations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ \\<YOUR SOLUTION HERE\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Leaky Integrate-and-Fire neurons\n",
    "\n",
    "**a) Computing gain and bias.** As in the second part of 1.1a), given a maximum firing rate $a^\\mathrm{max}$ and a bias $J^\\mathrm{bias}$, write down the equations for computing $\\alpha$ and the $J^\\mathrm{bias}$ for this specific neuron model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ \\<YOUR SOLUTION HERE\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Neuron tuning curves.** Generate the same plot as in 1.1b). Use $\\tau_\\mathrm{ref}=2 \\mathrm{ms}$ and $\\tau_{RC}=20 \\mathrm{ms}$. Use the same distribution of $x$-intercepts and maximum firing rates as in 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Impact of noise.** Generate the same four plots as in 1.1f) (adding/not adding noise to $A$, accounting/not accounting for noise when computing $\\vec d$), and report the RMSE both with and without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reperesentation of Vectors\n",
    "\n",
    "## 2.1 Vector tuning curves\n",
    "\n",
    "**a) Plotting 2D tuning curves.** Plot the tuning curve of an LIF neuron whose 2D preferred direction vector is at an angle of $\\theta=-\\pi/4$, has an $x$-intercept at the origin $(0,0)$, and has a maximum firing rate of $100 \\mathrm{Hz}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Plotting the 2D tuning curve along the unit circle.** Plot the tuning curve for the same neuron as in a), but only considering the points around the unit circle, i.e., sample the activation for different angles $\\theta$. Fit a curve of the form $c_1 \\cos(c_2\\theta+c_3)+c_4$ to the tuning curve and plot it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Discussion.** What makes a cosine a good choice for the curve fit in 2.1b? Why does it differ from the ideal curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ \\<YOUR SOLUTION HERE\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Vector representation\n",
    "\n",
    "**a) Choosing encoding vectors.** Generate a set of $100$ random unit vectors uniformly distributed around the unit circle. These will be the encoders $\\vec e$ for $100$ neurons. Plot these vectors with a quiver or line plot (i.e., not just points, but lines/arrows to the points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Computing the identity decoder.** Use LIF neurons with the same properties as in question 1.3. When computing the decoders, take into account noise with $\\sigma = 0.2\\max(A)$. Plot the decoders in the same way you plotted the encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Discussion.** How do these decoding vectors compare to the encoding vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ \\<YOUR SOLUTION HERE\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) Testing the decoder.** Generate 20 random $\\vec x$ values throughout the unit circle (i.e.,~with different directions and radiuses). For each $\\vec x$ value, determine the neural activity $a_i$ for each of the 100 neurons. Now decode these values (i.e. compute $\\hat{x} = D \\vec a$) using the decoders from part b). Plot the original and decoded values on the same graph in different colours, and compute the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) Using encoders as decoders.** Repeat part d) but use the *encoders* as decoders. This is what Georgopoulos used in his original approach to decoding information from populations of neurons. Plot the decoded values and compute the RMSE. In addition, recompute the RMSE in both cases, but ignore the magnitude of the decoded vectors by normalizing before computing the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍ <YOUR SOLUTION HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f) Discussion.** When computing the RMSE on the normalized vectors, using the encoders as decoders should result in a larger, yet still surprisingly small error. Thinking about random unit vectors in high dimensional spaces, why is this the case? What are the relative merits of these two approaches to decoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ \\<YOUR SOLUTION HERE\\>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
