---
title: "Assignment 4"
author: "Fenglin Chen"
date: "05/12/2021"
output: word_document
---

### Question 1 a)

```{r}
econ <- read.csv("EconomicMobility.csv", header=TRUE)

VarIQR <- function(pop) {
  pop.var <- sum((pop - mean(pop))^2) / length(pop)
  pop.iqr <- IQR(pop)
  c(pop.var, pop.iqr)
}
pop.res <- VarIQR(econ$Mobility)
pop.res
```
    
The population variance is `0.002768671` and the population IQR is `0.053418368`.

\newpage
### Question 1 b)

```{r}
M <- 1000
n <- 100
set.seed(341)
samples <- sapply(1:M, FUN = function(m) sample(econ$Mobility, n, replace=FALSE))

samp.res <- apply(samples, MARGIN=2, FUN=VarIQR)
samp.var.err <- samp.res[1,] - pop.res[1]
samp.iqr.err <- samp.res[2,] - pop.res[2]

par(mfrow=c(1,2), mar=c(2.5, 2.5, 1.5, 0), oma=c(2, 2, 0, 0))
hist(samp.var.err, breaks=20, main='Variance')
hist(samp.iqr.err, breaks=20, main='IQR')
mtext("Sample Error", side=1, line=0, outer=TRUE, cex=1)
mtext("Frequency", side=2, line=0, outer=TRUE, cex=1, las=0)
```

\newpage
### Question 1 c)

```{r, echo=FALSE}
CommunitiesSample = c(265, 596, 270, 334, 653, 273,  93,  58, 113, 668, 235, 243, 703, 672, 411, 231, 723, 
                      127, 640, 217, 626, 279, 482, 395, 410, 162,   7, 603,  28, 100,  68, 141, 593, 564,
                      557, 604, 443, 202, 480, 285, 210, 585, 199, 224, 577, 551, 464, 611, 292, 649,  80,
                      180,   3, 463, 479,  77, 453, 241, 548, 488, 447, 396, 124, 552, 340, 615,  63, 380,
                      599, 590, 386,  99, 374, 225, 116, 610, 215, 651,  55, 563, 562, 122, 476, 355,  36,
                      293, 534, 652,  53, 571, 398, 353, 383, 627, 352, 377, 537, 151, 392,  51)
```

#### i)

```{r}
comm.samp <- econ$Mobility[CommunitiesSample]
comm.samp.res <- VarIQR(comm.samp)
comm.samp.res
```

The community sample variance is `0.003185652`, and the IQR is `0.051400134`.

#### ii)

```{r}
B <- 1000
n <- length(comm.samp)
set.seed(341)
boot.samples <- sapply(1:B, FUN = function(m) sample(comm.samp, n, replace=TRUE))

boot.samp.res <- apply(boot.samples, MARGIN=2, FUN=VarIQR)
boot.samp.var.err <- boot.samp.res[1,] - comm.samp.res[1]
boot.samp.iqr.err <- boot.samp.res[2,] - comm.samp.res[2]

par(mfrow=c(1,2), mar=c(2.5, 2.5, 1.5, 0), oma=c(2, 2, 0, 0))
hist(boot.samp.var.err, breaks=20, main='Variance')
hist(boot.samp.iqr.err, breaks=20, main='IQR')
mtext("Bootstrap Sample Error", side=1, line=0, outer=TRUE, cex=1)
mtext("Frequency", side=2, line=0, outer=TRUE, cex=1, las=0)
```

#### iii)

```{r}
se.var <- sd(boot.samp.res[1,])
se.iqr <- sd(boot.samp.res[2,])
c(se.var, se.iqr)
```
    
The standard error of the sample estimate of the variance is `0.0007497525`, and `0.0090446275` for IQR.

```{r}
quantile(boot.samp.res[1,], probs=c(0.025, 0.975))
quantile(boot.samp.res[2,], probs=c(0.025, 0.975))
```
    
The 95% confidence interval for the population variance and IQR via the percentile method are `[0.001889944, 0.004726023]` and `[0.04272616, 0.08065662]` respectively. The population attributes are indeed within these intervals, which validates the results.

\newpage
### Question 1 d)

```{r}
set.seed(341)
numIntervals <- 100
fit.var <- 0
fit.iqr <- 0
for (i in 1:numIntervals){
  B <- 1000
  n <- 100
  one.samp <- sample(econ$Mobility, n, replace=FALSE)
  boot.samps <- sapply(1:B, FUN = function(m) sample(one.samp, n, replace=TRUE))
  boot.samps.res <- apply(boot.samps, MARGIN=2, FUN=VarIQR)
  boot.samps.var <- boot.samps.res[1,]
  boot.samps.iqr <- boot.samps.res[2,]
  var.percent <- quantile(boot.samps.var, probs=c(0.025, 0.975))
  iqr.percent <- quantile(boot.samps.iqr, probs=c(0.025, 0.975))
  if (pop.res[1] >= var.percent[1] && pop.res[1] <= var.percent[2]) {
    fit.var <- fit.var + 1
  }
  if (pop.res[2] >= iqr.percent[1] && pop.res[2] <= iqr.percent[2]) {
    fit.iqr <- fit.iqr + 1
  }
}
c(fit.var, fit.iqr)
```
    
The 95% percentile-based confidence interval coverage probability of variance is about `84%` , and the coverage probability of IQR is about `95%`.

```{r}
sdN = function(x) {
  sqrt(var(x) * (length(x) - 1)/length(x))
}

temp.var <- rep(0, numIntervals)
temp.var[1:fit.var] <- 1
sdN(temp.var) / sqrt(numIntervals)

temp.iqr <- rep(0, numIntervals)
temp.iqr[1:fit.iqr] <- 1
sdN(temp.iqr) / sqrt(numIntervals)
```

The coverage probability is the count of `TRUE/FALSE` divided by the number of iterations, so it is a mean. The standard error of a mean is the standard deviation divided by the square root of the sample size.    
    
The coverage probability of variance was far below what was predicted (84 instead of 95) and with a higher standard error (3.666%). Therefore, this attribute is more sensitive to the effects of sampling and bootstrapping. On the other hand, IQR had exactly the predicted coverage probability with a much smaller standard error (2.179%), so it is better suited for bootstrapping.

\newpage
### Question 2 a)

```{r}
getmuhat <- function(sampleXY, complexity = 1) {
  formula <- paste0("y ~ ",
                    if (complexity==0) { "1" } 
                    else paste0("poly(x, ", complexity, ", raw = FALSE)") )
  fit <- lm(as.formula(formula), data = sampleXY)
  tx = sampleXY$x
  ty = fit$fitted.values
  range.X = range(tx)
  val.rY  = c( mean(ty[tx == range.X[1]]), 
               mean(ty[tx == range.X[2]]) )
  muhat <- function(x){
    if ("x" %in% names(x)) { newdata <- x } 
    else { newdata <- data.frame(x = x) }
    val = predict(fit, newdata = newdata)
    val[newdata$x < range.X[1]] = val.rY[1]
    val[newdata$x > range.X[2]] = val.rY[2]
    val
  }
  muhat
}

ozone <- read.csv("OzoneData.csv", header=TRUE)
names(ozone)  <- c("x", "y")
```

```{r}
par(mfrow = c(3, 2), mar = 2.5 * c(0.75, 1, 1, 0.1))
degrees <- c(1, 2, 5, 10, 15, 20)
colours <- c(2, 3, 4, 5, 6, 7)
for (i in 1:6) {
  muhat <- getmuhat(ozone, complexity = degrees[i])
  plot(y ~ x, data = ozone, main = paste("Degree ", degrees[i]), pch = 19,
       xlab = "Day", ylab = "Ozone Level", col = adjustcolor("black", 0.4))
  curve(muhat, add = TRUE, lwd = 2, col = colours[i])
  legend(-10, 40, legend=paste("Degree ", degrees[i]), col = colours[i], cex=0.8, lty=1)
}
```

\newpage
### Question 2 b)

```{r}
popSize <- function(pop) {nrow(as.data.frame(pop))}
getSampleComp <- function(pop, size, replace=FALSE) {
  N <- popSize(pop)
  samp <- rep(FALSE, N)
  samp[sample(1:N, size, replace = replace)] <- TRUE
  samp
}

M <- 50
n <- 100
set.seed(341)
samples <- lapply(1:M, FUN = function(i) {getSampleComp(ozone, n)})
Ssam <- lapply(samples, FUN = function(Si) {ozone[Si,]})
Tsam <- lapply(samples, FUN = function(Si) {ozone[!Si,]})

par(mfrow = c(3, 2), mar = 2.5 * c(0.75, 1, 1, 0.1))
degrees <- c(1, 2, 5, 10, 15, 20)
colours <- c(2, 3, 4, 5, 6, 7)
for (i in 1:6) {
  fits <- lapply(Ssam, FUN=getmuhat, complexity = degrees[i])
  plot(y ~ x, data = ozone, main = paste("Degree ", degrees[i]), pch = 19,
       xlab = "Day", ylab = "Ozone Level", col = adjustcolor("black", 0.4))
  for (fit in fits) {
    curve(fit, add = TRUE, lwd = 2, col = colours[i])
  }
  legend(-10, 40, legend=paste("Degree ", degrees[i]), col = colours[i], cex=0.8, lty=1)
}
```

\newpage
### Question 2 c)

```{r}
library(knitr)
getmuFun <- function(pop) {
  pop = na.omit(pop)
  tauFun = approxfun(pop$x, pop$y, rule = 2, ties = mean)
  tauFun
}

getmubar <- function(muhats) {
  function(x) {
    Ans <- sapply(muhats, FUN = function(muhat) { muhat(x) })
    apply(Ans, MARGIN = 1, FUN = mean)
  }
}

apse_all <- function(Ssamples, Tsamples, complexity, tau) {
  N_S <- length(Ssamples)
  muhats <- lapply(Ssamples, FUN = function(sample) getmuhat(sample, complexity))
  mubar <- getmubar(muhats)
  
  rowMeans(sapply(1:N_S, FUN = function(j) {
    T_j <- Tsamples[[j]]
    S_j <- Ssamples[[j]]
    muhat <- muhats[[j]]
    T_j <- na.omit(T_j)
    y <- c(S_j$y, T_j$y)
    x <- c(S_j$x, T_j$x)
    tau_x <- tau(x)
    muhat_x <- muhat(x)
    mubar_x <- mubar(x)
    apse <- (y - muhat_x)
    bias2 <- (mubar_x - tau_x)
    var_mutilde <- (muhat_x - mubar_x)
    var_y <- (y - tau_x)
    squares <- rbind(apse, var_mutilde, bias2, var_y)^2
    rowMeans(squares)
  }))
}

degrees <- 0:15
tauFun = getmuFun(ozone)
apse_vals <- sapply(degrees, FUN = function(deg) {
  apse_all(Ssam, Tsam, complexity = deg, tau = tauFun)
})
colnames(apse_vals) = paste("deg", degrees, sep = " ")
```
```{r, eval=FALSE}
kable(round(apse_vals, 3))
```
```{r, echo=FALSE}
kable(round(apse_vals[,0:6], 3), "pipe")
kable(round(apse_vals[,7:12], 3), "pipe")
kable(round(apse_vals[,13:16], 3), "pipe")
```

\newpage
### Question 2 d)

```{r}
plot(degrees, sqrt(apse_vals[3,]), main="Errors for Line Fits", xlab = "Degree", 
     ylab = "sqrt(Error)", type = "l", ylim = c(0, 13), col = 3, lwd = 2)
lines(degrees, sqrt(apse_vals[2,]), col = 2, lwd = 2)
lines(degrees, sqrt(apse_vals[1,]), col = 1, lwd = 2)
lines(degrees, sqrt(apse_vals[4,]), col = 4, lwd = 2)
legend(10, 13, legend=c("APSE", "Var_Mu-tilde", "Bias^2", "Var_Y"), 
       col = 1:4, cex=0.65, lty=1)
```
    
The plot shows that APSE decreases dramatically from `deg=2` to `deg=3`, mainly due to decreasing `bias^2`. It is lowest at `deg=9`, but increases slightly after due to increasing `var_mutilde`. Clearly, there is a trade-off between bias and variance. On the other hand, `var_y` stays constant at 0 for the entire plot.

\newpage
### Question 2 e)

Degree 9 has the best predictive accuracy because it has the lowest APSE.

```{r}
muhat <- getmuhat(ozone, complexity = 9)
plot(y ~ x, data = ozone, main = "Degree 9", pch = 19,
     xlab = "Day", ylab = "Ozone Level", col = adjustcolor("black", 0.4))
curve(muhat, add = TRUE, lwd = 2, col = 2)
```