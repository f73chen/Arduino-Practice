{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import ale_py\n",
    "\n",
    "from option_critic import OptionCriticFeatures\n",
    "from attention_option_critic import AOCFeatures\n",
    "from fourrooms_env import Fourrooms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from highway_env.envs.intersection_env import IntersectionEnv\n",
    "# intersection_config = IntersectionEnv.default_config()\n",
    "# intersection_config[\"observation\"][\"flatten\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OC_CARTPOLE =  {'num_options': 6, 'temperature': 1.0021572199170095, 'epsilon_start': 0.8892105473942505, 'epsilon_min': 0.16269456380621444, 'epsilon_decay': 130754, 'gamma': 0.9607620430012502, 'tau': 0.8763187663144012, 'termination_reg': 0.00271802276023284, 'entropy_reg': 0.03354402472584794,                                                                                                                       'hidden_size': 64,  'state_size': 96, 'learning_rate': 0.0029659163964686295, 'batch_size': 128, 'critic_freq': 40, 'target_update_freq': 10, 'buffer_size': 2000}\n",
    "OC_FOURROOMS = {'num_options': 6, 'temperature': 1.0021572199170095, 'epsilon_start': 0.8892105473942505, 'epsilon_min': 0.16269456380621444, 'epsilon_decay': 130754, 'gamma': 0.9607620430012502, 'tau': 0.8763187663144012, 'termination_reg': 0.00271802276023284, 'entropy_reg': 0.03354402472584794,                                                                                                                       'hidden_size': 64,  'state_size': 96, 'learning_rate': 0.0029659163964686295, 'batch_size': 128, 'critic_freq': 40, 'target_update_freq': 10, 'buffer_size': 2000} # TODO\n",
    "\n",
    "AOC_CARTPOLE = {'num_options': 9, 'temperature': 0.7014647237611604, 'epsilon_start': 0.9266293878140235, 'epsilon_min': 0.1477058909278815,  'epsilon_decay': 696722, 'gamma': 0.8995605059962165, 'tau': 0.9876976682663541, 'termination_reg': 0.02136841630458894, 'entropy_reg': 0.0072884705990544025, 'diversity_reg': 0.03153052105606061, 'sparsity_reg': 0.006558493362079418, 'smoothness_reg': 0.007593783252036938, 'hidden_size': 112, 'state_size': 32, 'learning_rate': 0.001247281645969728,  'batch_size': 224, 'critic_freq': 80, 'target_update_freq': 15, 'buffer_size': 3000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_IDX = 2\n",
    "ENV_NAME =    [\"CartPole-v1\", \"LunarLander-v3\", \"Fourrooms\", \"highway-fast-v0\", \"intersection-v1\", \"racetrack-v0\", \"ALE/Pong-v5\"][ENV_IDX]\n",
    "ENV_TYPE =    [\"cartpole\",    \"lunarlander\",    \"fourrooms\", \"highway\",         \"intersection\",    \"racetrack\",    \"pong\"][ENV_IDX]\n",
    "RENDER_MODE = [\"human\",       \"human\",          \"human\",     \"rgb_array\",       \"rgb_array\",       \"rgb_array\",    \"human\"][ENV_IDX]\n",
    "TOTAL_TIMESTEPS = int(1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature input, Discrete action space\n",
    "    - \"CartPole-v1\"      # (4,) --> 2\n",
    "    - \"LunarLander-v3\"     # (8,) --> 4\n",
    "    - \"highway-fast-v0\"  # (5, 5) --> 5\n",
    "    - \"intersection-v0\"  # (15, 7) --> 3\n",
    "\n",
    "- Feature input, Continuous action space\n",
    "    - \"intersection-v1\"  # (5, 8) --> -1, 1, (2,)\n",
    "    - \"racetrack-v0\"     # (2, 12, 12)\n",
    "\n",
    "- Image input, Discrete action space\n",
    "    - \"ALE/Pong-v5\"    # (210, 160, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(env, env_type, model_type, train=False, total_timesteps=TOTAL_TIMESTEPS):\n",
    "    if model_type == \"aoc\":\n",
    "        model = AOCFeatures(\n",
    "            env=env,\n",
    "            num_options=AOC_CARTPOLE[\"num_options\"],\n",
    "            device=\"cpu\",\n",
    "            temperature=AOC_CARTPOLE[\"temperature\"],\n",
    "            epsilon_start=AOC_CARTPOLE[\"epsilon_start\"],\n",
    "            epsilon_min=AOC_CARTPOLE[\"epsilon_min\"],\n",
    "            epsilon_decay=AOC_CARTPOLE[\"epsilon_decay\"],\n",
    "            gamma=AOC_CARTPOLE[\"gamma\"],\n",
    "            tau=AOC_CARTPOLE[\"tau\"],\n",
    "            termination_reg=AOC_CARTPOLE[\"termination_reg\"],\n",
    "            entropy_reg=AOC_CARTPOLE[\"entropy_reg\"],\n",
    "            diversity_reg=AOC_CARTPOLE[\"diversity_reg\"],\n",
    "            sparsity_reg=AOC_CARTPOLE[\"sparsity_reg\"],\n",
    "            smoothness_reg=AOC_CARTPOLE[\"smoothness_reg\"],\n",
    "            hidden_size=AOC_CARTPOLE[\"hidden_size\"],\n",
    "            state_size=AOC_CARTPOLE[\"state_size\"],\n",
    "            learning_rate=AOC_CARTPOLE[\"learning_rate\"],\n",
    "            batch_size=AOC_CARTPOLE[\"batch_size\"],\n",
    "            critic_freq=AOC_CARTPOLE[\"critic_freq\"],\n",
    "            target_update_freq=AOC_CARTPOLE[\"target_update_freq\"],\n",
    "            buffer_size=AOC_CARTPOLE[\"buffer_size\"],\n",
    "            tensorboard_log=f\"results/{env_type}_{model_type}/\",\n",
    "            verbose=0,\n",
    "            testing=False\n",
    "        )\n",
    "        \n",
    "    elif model_type == \"oc\":\n",
    "        model = OptionCriticFeatures(\n",
    "            env=env,\n",
    "            num_options=OC_CARTPOLE[\"num_options\"],\n",
    "            device=\"cpu\",\n",
    "            temperature=OC_CARTPOLE[\"temperature\"],\n",
    "            epsilon_start=OC_CARTPOLE[\"epsilon_start\"],\n",
    "            epsilon_min=OC_CARTPOLE[\"epsilon_min\"],\n",
    "            epsilon_decay=OC_CARTPOLE[\"epsilon_decay\"],\n",
    "            gamma=OC_CARTPOLE[\"gamma\"],\n",
    "            tau=OC_CARTPOLE[\"tau\"],\n",
    "            termination_reg=OC_CARTPOLE[\"termination_reg\"],\n",
    "            entropy_reg=OC_CARTPOLE[\"entropy_reg\"],\n",
    "            hidden_size=OC_CARTPOLE[\"hidden_size\"],\n",
    "            state_size=OC_CARTPOLE[\"state_size\"],\n",
    "            learning_rate=OC_CARTPOLE[\"learning_rate\"],\n",
    "            batch_size=OC_CARTPOLE[\"batch_size\"],\n",
    "            critic_freq=OC_CARTPOLE[\"critic_freq\"],\n",
    "            target_update_freq=OC_CARTPOLE[\"target_update_freq\"],\n",
    "            buffer_size=OC_CARTPOLE[\"buffer_size\"],\n",
    "            tensorboard_log=f\"results/{env_type}_{model_type}/\",\n",
    "            verbose=0,\n",
    "            testing=False\n",
    "        )\n",
    "\n",
    "    if train:\n",
    "        model.learn(total_timesteps=total_timesteps)\n",
    "        model.save(f\"results/{env_type}_{model_type}/model\")\n",
    "    else:\n",
    "        model.load(f\"results/{env_type}_{model_type}/best_model\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This crashes if run in .ipynb instead of .py\n",
    "def simulate_env(model, env, num_episodes=10):\n",
    "    for episode in range(num_episodes):\n",
    "        done = truncated = False\n",
    "        obs, info = env.reset()\n",
    "        option = None\n",
    "        option_termination = True\n",
    "        while not (done or truncated):\n",
    "            option, action, logp, entropy = model.predict(obs, option, option_termination)\n",
    "            option_termination = model.get_option_termination(obs, option)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            env.render()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_primitive_actions_per_option(model, env, num_episodes=10):\n",
    "    num_actions = env.action_space.n\n",
    "    num_options = model.num_options\n",
    "    option_actions = np.zeros((num_actions, num_options))\n",
    "    \n",
    "    # Collect data for visualizations\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        done = truncated = False\n",
    "        obs, info = env.reset()\n",
    "        option = None\n",
    "        option_termination = True\n",
    "        while not (done or truncated):\n",
    "            option, action, logp, entropy = model.predict(obs, option, option_termination, deterministic=True)\n",
    "            option_actions[action, option] += 1\n",
    "            option_termination = model.get_option_termination(obs, option)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "    env.close()\n",
    "    \n",
    "    # Normalize across options\n",
    "    option_sums = option_actions.sum(axis=0)\n",
    "    norm_option_actions = np.zeros_like(option_actions, dtype=float)\n",
    "    for col in range(option_actions.shape[1]):\n",
    "        if option_sums[col] > 0:\n",
    "            norm_option_actions[:, col] = option_actions[:, col] / option_sums[col]\n",
    "\n",
    "    plt.figure(figsize=(num_options+1, num_actions))\n",
    "    plt.imshow(norm_option_actions, cmap=\"viridis\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Frequency\")\n",
    "    plt.xlabel(\"Options\")\n",
    "    plt.ylabel(\"Actions\")\n",
    "    plt.title(\"Distribution of Primitive Actions Per Option\")\n",
    "    plt.xticks(ticks=np.arange(num_options))\n",
    "    plt.yticks(ticks=np.arange(num_actions))\n",
    "    plt.show()\n",
    "    \n",
    "    # Normalize across actions\n",
    "    action_sums = option_actions.sum(axis=1)\n",
    "    norm_action_options = np.zeros_like(option_actions, dtype=float)\n",
    "    for row in range(option_actions.shape[0]):\n",
    "        if action_sums[row] > 0:\n",
    "            norm_action_options[row, :] = option_actions[row, :] / action_sums[row]\n",
    "\n",
    "    plt.figure(figsize=(num_options+1, num_actions))\n",
    "    plt.imshow(norm_action_options, cmap=\"plasma\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Frequency\")\n",
    "    plt.xlabel(\"Options\")\n",
    "    plt.ylabel(\"Actions\")\n",
    "    plt.title(\"Distribution of Options Per Action (Normalized by Actions)\")\n",
    "    plt.xticks(ticks=np.arange(num_options))\n",
    "    plt.yticks(ticks=np.arange(num_actions))\n",
    "    plt.show()\n",
    "    \n",
    "    # Histogram for total option usage\n",
    "    option_totals = option_actions.sum(axis=0)\n",
    "    total_usage = option_totals.sum()\n",
    "    option_percent = (option_totals / total_usage) * 100\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.bar(range(num_options), option_percent, color=\"steelblue\")\n",
    "    plt.xlabel(\"Options\")\n",
    "    plt.ylabel(\"Usage Percentage (%)\")\n",
    "    plt.title(\"Percentage Usage of Each Option\")\n",
    "    plt.xticks(ticks=np.arange(num_options))\n",
    "    plt.ylim(0, option_percent.max() + 5)\n",
    "    \n",
    "    # Display the percentage number above each bar\n",
    "    for i, bar in enumerate(bars):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, \n",
    "                 f\"{option_percent[i]:.2f}%\", ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENV_TYPE == \"fourrooms\":\n",
    "    env = Fourrooms()\n",
    "    render_env = Fourrooms(render_mode=RENDER_MODE)\n",
    "else:\n",
    "    env = gym.make(env_name=ENV_NAME)\n",
    "    render_env = gym.make(env_name=ENV_NAME, render_mode=RENDER_MODE)\n",
    "\n",
    "oc = get_model(env=env, env_type=ENV_TYPE, model_type=\"oc\", train=True)\n",
    "\n",
    "# simulate_env(oc, render_env, num_episodes=1)\n",
    "visualize_primitive_actions_per_option(oc, env=env, num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AOC attentions per option for each input feature\n",
    "def get_attention_per_feature(model, env, num_episodes=10):\n",
    "    num_features = model.in_features\n",
    "    num_options = model.num_options\n",
    "    option_attentions = np.zeros((num_options, num_features))\n",
    "    model.testing = True\n",
    "    \n",
    "    # Collect data for visualizations\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        done = truncated = False\n",
    "        obs, info = env.reset()\n",
    "        option = None\n",
    "        option_termination = True\n",
    "        while not (done or truncated):\n",
    "            option, action, logp, entropy = model.predict(obs, option, option_termination, deterministic=True)\n",
    "            \n",
    "            _, attention_mask = model.apply_attention(obs, option)\n",
    "            attention_mask = attention_mask.detach().cpu().numpy().squeeze()\n",
    "            option_attentions[option, :] += attention_mask\n",
    "            \n",
    "            option_termination = model.get_option_termination(obs, option)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "    env.close()\n",
    "    \n",
    "    # Normalize across options (feature contributions to each option sum to 1)\n",
    "    option_sums = option_attentions.sum(axis=1, keepdims=True)\n",
    "    norm_feature_per_option = np.zeros_like(option_attentions, dtype=float)\n",
    "    for row in range(option_attentions.shape[0]):\n",
    "        if option_sums[row] > 0:\n",
    "            norm_feature_per_option[row, :] = option_attentions[row, :] / option_sums[row]\n",
    "\n",
    "    plt.figure(figsize=(num_features + 1, num_options))\n",
    "    plt.imshow(norm_feature_per_option, cmap=\"viridis\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Attention Weight\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Options\")\n",
    "    plt.title(\"Normalized Attention Distribution (Features per Option)\")\n",
    "    plt.xticks(ticks=np.arange(num_features))\n",
    "    plt.yticks(ticks=np.arange(num_options))\n",
    "    plt.show()\n",
    "\n",
    "    # Normalize across features (option contributions to each feature sum to 1)\n",
    "    feature_sums = option_attentions.sum(axis=0, keepdims=True)\n",
    "    norm_option_per_feature = np.zeros_like(option_attentions, dtype=float)\n",
    "    for col in range(option_attentions.shape[1]):\n",
    "        if feature_sums[0, col] > 0:\n",
    "            norm_option_per_feature[:, col] = option_attentions[:, col] / feature_sums[0, col]\n",
    "\n",
    "    plt.figure(figsize=(num_features + 1, num_options))\n",
    "    plt.imshow(norm_option_per_feature, cmap=\"plasma\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Attention Weight\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Options\")\n",
    "    plt.title(\"Normalized Attention Distribution (Options per Feature)\")\n",
    "    plt.xticks(ticks=np.arange(num_features))\n",
    "    plt.yticks(ticks=np.arange(num_options))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ENV_TYPE == \"fourrooms\":\n",
    "#     env = Fourrooms()\n",
    "#     render_env = Fourrooms(render_mode=RENDER_MODE)\n",
    "# else:\n",
    "#     env = gym.make(env_name=ENV_NAME)\n",
    "#     render_env = gym.make(env_name=ENV_NAME, render_mode=RENDER_MODE)\n",
    "\n",
    "# aoc = get_model(env=env, env_type=ENV_TYPE, model_type=\"aoc\", train=False)\n",
    "\n",
    "# # simulate_env(model, env=render_env, num_episodes=1)\n",
    "# visualize_primitive_actions_per_option(aoc, env=env, num_episodes=100)\n",
    "# get_attention_per_feature(aoc, env=env, num_episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "highway",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
